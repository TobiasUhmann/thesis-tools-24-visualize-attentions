{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Visualize how well the class embeddings attend on words and\n",
    "sentences. The expected result would be that the “married”\n",
    "class embedding, for example, attends heavily on words and\n",
    "sentences related to marriage like “married”, “husband”, “wife”, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from data.power.split.split_dir import SplitDir\n",
    "\n",
    "from util import plot_tensor\n",
    "from pathlib import Path\n",
    "from random import shuffle\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from torch import tensor, Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dao.ower.ower_dir import Sample\n",
    "from data.power.samples.samples_dir import SamplesDir\n",
    "from data.power.texter_pkl import TexterPkl\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.precision', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "texter_pkl_path = '../data/power/texter-v2/context_attend_cde-irt-5-marked.pkl'\n",
    "\n",
    "# Input data\n",
    "samples_dir_path = '../data/power/samples-v5/cde-irt-5-marked/'\n",
    "class_count = 100\n",
    "sent_count = 5\n",
    "\n",
    "split_dir_path = '../data/power/split-v2/cde-0/'\n",
    "\n",
    "# Pre-processing\n",
    "sent_len = 64\n",
    "\n",
    "# Testing\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Check that (input) POWER Texter PKL exists\n",
    "#\n",
    "\n",
    "texter_pkl = TexterPkl(Path(texter_pkl_path))\n",
    "texter_pkl.check()\n",
    "\n",
    "#\n",
    "# Check that (input) POWER Samples Directory exists\n",
    "#\n",
    "\n",
    "samples_dir = SamplesDir(Path(samples_dir_path))\n",
    "samples_dir.check()\n",
    "\n",
    "#\n",
    "# Check that (input) Power Split Directory exists\n",
    "#\n",
    "\n",
    "split_dir = SplitDir(Path(split_dir_path))\n",
    "split_dir.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Texter and test data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "texter = texter_pkl.load().cpu()\n",
    "\n",
    "test_set = samples_dir.test_samples_tsv.load(class_count, sent_count)\n",
    "test_ent_to_sents = {sample.ent: sample.sents for sample in test_set}\n",
    "\n",
    "\n",
    "def generate_batch(batch: List[Sample]) -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    :param    batch:            [Sample(ent, ent_lbl, [class], [sent])]\n",
    "\n",
    "    :return:  ent_batch:        IntTensor[batch_size],\n",
    "              tok_lists_batch:  IntTensor[batch_size, sent_count, sent_len],\n",
    "              masks_batch:      IntTensor[batch_size, sent_count, sent_len],\n",
    "              classes_batch:    IntTensor[batch_size, class_count]\n",
    "    \"\"\"\n",
    "\n",
    "    ent_batch, _, classes_batch, sents_batch = zip(*batch)\n",
    "\n",
    "    for sents in sents_batch:\n",
    "        shuffle(sents)\n",
    "\n",
    "    flat_sents_batch = [sent for sents in sents_batch for sent in sents]\n",
    "\n",
    "    encoded = texter.tokenizer(flat_sents_batch, padding=True, truncation=True, max_length=sent_len,\n",
    "                               return_tensors='pt')\n",
    "\n",
    "    b_size = len(ent_batch)  # usually b_size == batch_size, except for last batch in samples\n",
    "    tok_lists_batch = encoded.input_ids.reshape(b_size, sent_count, -1)\n",
    "    masks_batch = encoded.attention_mask.reshape(b_size, sent_count, -1)\n",
    "\n",
    "    return tensor(ent_batch), tok_lists_batch, masks_batch, tensor(classes_batch)\n",
    "\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, collate_fn=generate_batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Debug Info"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load class info\n",
    "rel_tail_freq_lbl_list = samples_dir.classes_tsv.load()\n",
    "\n",
    "# load test ent labels\n",
    "test_ent_to_lbl = split_dir.test_entities_tsv.load()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predict test entities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "limit_classes = 4\n",
    "\n",
    "texter.eval()\n",
    "\n",
    "for i, (ent_batch, sents_batch, masks_batch, gt_batch) in enumerate(test_loader):\n",
    "    if i == 1:\n",
    "        break\n",
    "\n",
    "    # print('ent_batch', ent_batch)\n",
    "    # print('sent_batch', sents_batch)\n",
    "    # print('masks_batch', masks_batch)\n",
    "    # print('gt_batch', gt_batch)\n",
    "\n",
    "    logits_batch, atts_batch, = texter(sents_batch, masks_batch)\n",
    "    no_att_logits_batch = texter.forward_without_attention(sents_batch, masks_batch)\n",
    "\n",
    "    # print('logits_batch', logits_batch)\n",
    "    # print('atts_batch', atts_batch)\n",
    "\n",
    "    for ent, sents, masks, gt, logits, atts, no_att_logits in \\\n",
    "            zip(ent_batch, sents_batch, masks_batch, gt_batch, logits_batch, atts_batch, no_att_logits_batch):\n",
    "        print(test_ent_to_lbl[ent.item()])\n",
    "\n",
    "        print('sents')\n",
    "        pprint(test_ent_to_sents[ent.item()])\n",
    "\n",
    "        # print('masks')\n",
    "        # pprint(masks)\n",
    "\n",
    "        print('gt')\n",
    "        pprint(gt[:limit_classes])\n",
    "\n",
    "        print('logits')\n",
    "        pprint(logits[:limit_classes])\n",
    "\n",
    "        print('atts')\n",
    "        pprint(atts[:limit_classes])\n",
    "\n",
    "        class_labels = [rel_tail_freq_lbl_list[c][3] for c in range(class_count)][:4]\n",
    "        sent_labels = [f'sent {s}' for s in range(sent_count)]\n",
    "        plot_tensor(atts[:limit_classes], 'atts', [class_labels, sent_labels])\n",
    "\n",
    "        print('no_att_logits')\n",
    "        pprint(no_att_logits[:,:limit_classes])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}