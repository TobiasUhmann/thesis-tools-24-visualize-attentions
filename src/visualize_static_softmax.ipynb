{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Visualize how well the class embeddings attend on words and\n",
    "sentences. The expected result would be that the “married”\n",
    "class embedding, for example, attends heavily on words and\n",
    "sentences related to marriage like “married”, “husband”, “wife”, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "from random import shuffle\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.core.display import display, HTML\n",
    "from jinja2 import Template\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torch import tensor, Tensor\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dao.ower.ower_dir import Sample, OwerDir\n",
    "from ower.classifier import Classifier, debug\n",
    "from util import plot_tensor\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.precision', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Input data\n",
    "ower_dir_path = '../data/ower-v4/cde-irt-100-5/'\n",
    "class_count = 100\n",
    "sent_count = 5\n",
    "\n",
    "# Pre-processing\n",
    "sent_len = 64\n",
    "\n",
    "# Model\n",
    "emb_size = None\n",
    "vectors = 'glove.6B.300d'\n",
    "\n",
    "# Training\n",
    "batch_size = 1024\n",
    "device = 'cuda'\n",
    "epoch_count = 20\n",
    "lr = 0.1\n",
    "\n",
    "# Logging\n",
    "log_dir = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Check that (input) OWER Directory exists\n",
    "\n",
    "ower_dir = OwerDir(Path(ower_dir_path))\n",
    "ower_dir.check()\n",
    "\n",
    "## Load datasets\n",
    "\n",
    "train_set: List[Sample]\n",
    "valid_set: List[Sample]\n",
    "\n",
    "if emb_size is not None:\n",
    "    train_set, valid_set, _, vocab = ower_dir.read_datasets(class_count, sent_count)\n",
    "else:\n",
    "    train_set, valid_set, _, vocab = ower_dir.read_datasets(class_count, sent_count, vectors)\n",
    "\n",
    "## Create dataloaders\n",
    "\n",
    "def generate_batch(batch: List[Sample]) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "\n",
    "    ent_batch, gt_classes_batch, tok_lists_batch = zip(*batch)\n",
    "\n",
    "    cropped_tok_lists_batch = [[tok_list[:sent_len]\n",
    "                                for tok_list in tok_lists] for tok_lists in tok_lists_batch]\n",
    "\n",
    "    padded_tok_lists_batch = [[tok_list + [0] * (sent_len - len(tok_list))\n",
    "                               for tok_list in tok_lists] for tok_lists in cropped_tok_lists_batch]\n",
    "\n",
    "    for padded_tok_lists in padded_tok_lists_batch:\n",
    "        shuffle(padded_tok_lists)\n",
    "\n",
    "    return tensor(ent_batch), tensor(padded_tok_lists_batch), tensor(gt_classes_batch)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, collate_fn=generate_batch, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Calc class weights\n",
    "\n",
    "_, train_classes_stack, _ = zip(*train_set)\n",
    "train_classes_stack = numpy.array(train_classes_stack)\n",
    "train_freqs = train_classes_stack.mean(axis=0)\n",
    "\n",
    "class_weights = tensor(1 / train_freqs).to(device)\n",
    "\n",
    "## Create classifier\n",
    "\n",
    "if emb_size is not None:\n",
    "    classifier = Classifier.from_random(len(vocab), emb_size, class_count).to(device)\n",
    "else:\n",
    "    classifier = Classifier.from_pre_trained(vocab, class_count).to(device)\n",
    "\n",
    "debug['enabled'] = True\n",
    "\n",
    "optimizer = Adam(classifier.parameters(), lr=lr)\n",
    "criterion = BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "\n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "log_first_batch = False\n",
    "\n",
    "for epoch in range(epoch_count):\n",
    "\n",
    "    ## Train\n",
    "\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # Valid gt/pred classes across all batches\n",
    "    train_gt_classes_stack: List[List[int]] = []\n",
    "    train_pred_classes_stack: List[List[int]] = []\n",
    "\n",
    "    for batch_idx, (_, sents_batch, gt_classes_batch) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch}')):\n",
    "        sents_batch = sents_batch.to(device)\n",
    "        gt_classes_batch = gt_classes_batch.to(device)\n",
    "\n",
    "        logits_batch = classifier(sents_batch)\n",
    "\n",
    "        loss = criterion(logits_batch, gt_classes_batch.float())\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred_classes_batch = (logits_batch > 0).int()\n",
    "\n",
    "        train_gt_classes_stack += gt_classes_batch.cpu().numpy().tolist()\n",
    "        train_pred_classes_stack += pred_classes_batch.cpu().numpy().tolist()\n",
    "\n",
    "        #\n",
    "        # Log first batch\n",
    "        #\n",
    "\n",
    "        if log_first_batch and batch_idx == 0:\n",
    "\n",
    "            dlb = logits_batch.cpu().detach().numpy()  # logits batch\n",
    "            dpb = pred_classes_batch.cpu().detach().numpy()  # predicted classes batch\n",
    "            dgb = gt_classes_batch.cpu().detach().numpy()  # ground truth classes batch\n",
    "            dsb = sents_batch.cpu().detach().numpy()  # sentences batch\n",
    "\n",
    "            df_cols = ['entity', 'logits', 'p', 'gt', 'sents']\n",
    "            df_data = [('foo', logits, pred_classes, classes, [[vocab.itos[tok] for tok in sent] for sent in sents])\n",
    "                       for logits, pred_classes, classes, sents in zip(dlb, dpb, dgb, dsb)]\n",
    "\n",
    "            df = pd.DataFrame(df_data[:8], columns=df_cols)\n",
    "            display(df)\n",
    "\n",
    "            display_atts = debug['atts_batch'][:8].cpu()\n",
    "            ent_labels = [f'ent {i}' for i in range(batch_size)]\n",
    "            class_labels = [f'clss {i}' for i in range(class_count)]\n",
    "            sent_labels = [f'sent {i}' for i in range(sent_count)]\n",
    "            plot_tensor(display_atts, 'atts_batch', [ent_labels, class_labels, sent_labels])\n",
    "\n",
    "    ## Validate\n",
    "\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    # Valid gt/pred classes across all batches\n",
    "    valid_gt_classes_stack: List[List[int]] = []\n",
    "    valid_pred_classes_stack: List[List[int]] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (ent_batch, sents_batch, gt_classes_batch) in enumerate(tqdm(valid_loader, desc=f'Epoch {epoch}')):\n",
    "            sents_batch = sents_batch.to(device)\n",
    "            gt_classes_batch = gt_classes_batch.to(device)\n",
    "\n",
    "            logits_batch = classifier(sents_batch)\n",
    "            foo_logits_batch = classifier.foo(sents_batch)\n",
    "\n",
    "            loss = criterion(logits_batch, gt_classes_batch.float())\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            pred_classes_batch = (logits_batch > 0).int()\n",
    "\n",
    "            valid_gt_classes_stack += gt_classes_batch.cpu().numpy().tolist()\n",
    "            valid_pred_classes_stack += pred_classes_batch.cpu().numpy().tolist()\n",
    "\n",
    "            #\n",
    "            # Print first batch\n",
    "            #\n",
    "\n",
    "            if log_first_batch and batch_idx == 0:\n",
    "\n",
    "                dlb = logits_batch.cpu().detach().numpy()  # logits batch\n",
    "                dpb = pred_classes_batch.cpu().detach().numpy()  # predicted classes batch\n",
    "                dgb = gt_classes_batch.cpu().detach().numpy()  # ground truth classes batch\n",
    "                dsb = sents_batch.cpu().detach().numpy()  # sentences batch\n",
    "\n",
    "                df_cols = ['entity', 'logits', 'p', 'gt', 'sents']\n",
    "                df_data = [('foo', logits, pred_classes, classes, [[vocab.itos[tok] for tok in sent] for sent in sents])\n",
    "                           for logits, pred_classes, classes, sents in zip(dlb, dpb, dgb, dsb)]\n",
    "\n",
    "                df = pd.DataFrame(df_data[:8], columns=df_cols)\n",
    "                display(df)\n",
    "\n",
    "                display_atts = debug['atts_batch'][:8].cpu()\n",
    "                ent_labels = [f'ent {i}' for i in range(batch_size)]\n",
    "                class_labels = [f'clss {i}' for i in range(class_count)]\n",
    "                sent_labels = [f'sent {i}' for i in range(sent_count)]\n",
    "                plot_tensor(display_atts, 'atts_batch', [ent_labels, class_labels, sent_labels])\n",
    "\n",
    "\n",
    "    ## Log loss\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    valid_loss /= len(valid_loader)\n",
    "\n",
    "    writer.add_scalars('loss', {'train': train_loss, 'valid': valid_loss}, epoch)\n",
    "\n",
    "    ## Log metrics for most/least common classes\n",
    "\n",
    "    # tps = train precisions, vps = valid precisions, etc.\n",
    "    tps = precision_score(train_gt_classes_stack, train_pred_classes_stack, average=None)\n",
    "    vps = precision_score(valid_gt_classes_stack, valid_pred_classes_stack, average=None)\n",
    "    trs = recall_score(train_gt_classes_stack, train_pred_classes_stack, average=None)\n",
    "    vrs = recall_score(valid_gt_classes_stack, valid_pred_classes_stack, average=None)\n",
    "    tfs = f1_score(train_gt_classes_stack, train_pred_classes_stack, average=None)\n",
    "    vfs = f1_score(valid_gt_classes_stack, valid_pred_classes_stack, average=None)\n",
    "\n",
    "    # Log metrics for each class c\n",
    "    for c, (tp, vp, tr, vr, tf, vf), in enumerate(zip(tps, vps, trs, vrs, tfs, vfs)):\n",
    "\n",
    "        # many classes -> log only first and last ones\n",
    "        if (class_count > 2 * 3) and (3 <= c <= len(tps) - 3 - 1):\n",
    "            continue\n",
    "\n",
    "        writer.add_scalars('precision', {f'train_{c}': tp}, epoch)\n",
    "        writer.add_scalars('precision', {f'valid_{c}': vp}, epoch)\n",
    "        writer.add_scalars('recall', {f'train_{c}': tr}, epoch)\n",
    "        writer.add_scalars('recall', {f'valid_{c}': vr}, epoch)\n",
    "        writer.add_scalars('f1', {f'train_{c}': tf}, epoch)\n",
    "        writer.add_scalars('f1', {f'valid_{c}': vf}, epoch)\n",
    "\n",
    "    ## Log macro metrics over all classes\n",
    "\n",
    "    # mtp = mean train precision, mvp = mean valid precision, etc.\n",
    "    mtp = tps.mean()\n",
    "    mvp = vps.mean()\n",
    "    mtr = trs.mean()\n",
    "    mvr = vrs.mean()\n",
    "    mtf = tfs.mean()\n",
    "    mvf = vfs.mean()\n",
    "\n",
    "    writer.add_scalars('precision', {'train': mtp}, epoch)\n",
    "    writer.add_scalars('precision', {'valid': mvp}, epoch)\n",
    "    writer.add_scalars('recall', {'train': mtr}, epoch)\n",
    "    writer.add_scalars('recall', {'valid': mvr}, epoch)\n",
    "    writer.add_scalars('f1', {'train': mtf}, epoch)\n",
    "    writer.add_scalars('f1', {'valid': mvf}, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calc top class-word attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class_embs = classifier.class_embs\n",
    "tok_embs = classifier.embedding_bag.weight\n",
    "\n",
    "tok_atts = torch.einsum('ce, ve -> cv', class_embs, tok_embs)\n",
    "result = tok_atts.sort(descending=True)\n",
    "indices = result.indices.cpu().numpy()\n",
    "values = result.values.cpu().detach().numpy()\n",
    "\n",
    "rel_tail_freq_lbl_tuples = ower_dir.classes_tsv.load()\n",
    "_, _, _, class_labels = zip(*rel_tail_freq_lbl_tuples)\n",
    "\n",
    "for c, c_lbl in zip(range(class_count), class_labels):\n",
    "    print('\\n', c_lbl)\n",
    "    for tok, val in zip(indices[c][:10], values[c][:10]):\n",
    "        print('\\t{} ({:.2f})'.format(vocab.itos[tok], val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize class-word attentions in sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "limit_classes = 4\n",
    "\n",
    "tok_atts /= max(-tok_atts.min(), tok_atts.max())\n",
    "tok_atts *= 512\n",
    "tok_atts += 192\n",
    "\n",
    "def get_color(att: float) -> str:\n",
    "    att = max(min(att, 255), 0)\n",
    "    r, g, b = pyplot.get_cmap('viridis').colors[int(att)]\n",
    "\n",
    "    return f'rgba({int(r * 256)}, {int(g * 256)}, {int(b * 256)}, 0.5)'\n",
    "\n",
    "\n",
    "def render_sent(class_: int, sent: List[int]) -> str:\n",
    "\n",
    "    words = ['<span style=\"background-color: {}\">{}</span>'.format(\n",
    "                get_color(tok_atts[class_][tok]),\n",
    "                vocab.itos[tok] if tok != 0 else '_'\n",
    "            ) for tok in sent]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def render_table(sents: List[List[int]]) -> None:\n",
    "    short_class_labels = [class_label[-20:] for class_label in class_labels[:limit_classes]]\n",
    "\n",
    "    display(HTML(Template('''\n",
    "        <style>\n",
    "            table.atts td { text-align: left }\n",
    "        </style>\n",
    "\n",
    "        <table class='atts'>\n",
    "            <tr>\n",
    "                <th></th>\n",
    "\n",
    "                {% for i in range(len(sents)) %}\n",
    "                <th> Sent {{ i }} </th>\n",
    "                {% endfor %}\n",
    "            </tr>\n",
    "\n",
    "            {% for c in range(len(class_labels)) %}\n",
    "            <tr>\n",
    "                <th>{{ class_labels[c] }}</th>\n",
    "\n",
    "                {% for sent in sents %}\n",
    "                <td>{{ render_sent(c, sent) }}</td>\n",
    "                {% endfor %}\n",
    "            </tr>\n",
    "            {% endfor %}\n",
    "        </table>\n",
    "    ''').render(\n",
    "        sents=sents,\n",
    "        class_labels=short_class_labels,\n",
    "        render_sent=render_sent,\n",
    "        len=len\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ent_to_lbl = ower_dir.ent_labels_txt.load()\n",
    "\n",
    "logits_batch = logits_batch.cpu()\n",
    "foo_logits_batch = foo_logits_batch.cpu()\n",
    "pred_classes_batch = pred_classes_batch.cpu()\n",
    "gt_classes_batch = gt_classes_batch.cpu()\n",
    "atts_batch = debug['atts_batch'].cpu()\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    # gtcb = gt_classes_batch\n",
    "    # if not (gtcb[i][0] == gtcb[i][1] == gtcb[i][2] == gtcb[i][3] == 1):\n",
    "    #     continue\n",
    "\n",
    "    ent = ent_batch[i].item()\n",
    "\n",
    "    display(HTML('<h1>{} ({})</h1>'.format(ent_to_lbl[ent], ent)))\n",
    "\n",
    "    texts = [' '.join([vocab.itos[tok] if tok != 0 else '_' for tok in tok_list])\n",
    "             for tok_list in sents_batch[i]]\n",
    "\n",
    "    display(HTML(Template('''\n",
    "        <ul>\n",
    "            {% for text in texts %}\n",
    "                <li> {{ text }} </li>\n",
    "            {% endfor %}\n",
    "        </ul>\n",
    "    ''').render(texts=texts)))\n",
    "\n",
    "    print(class_labels)\n",
    "    print('logits =', logits_batch[i][:limit_classes])\n",
    "    print('foo_logits =', foo_logits_batch[i][:,:limit_classes])\n",
    "    print('pred =', pred_classes_batch[i][:limit_classes])\n",
    "    print('gt =', gt_classes_batch[i][:limit_classes])\n",
    "\n",
    "    sent_labels = [f'sent {i}' for i in range(sent_count)]\n",
    "    plot_tensor(atts_batch[i][:limit_classes], 'atts', [class_labels[:limit_classes], sent_labels])\n",
    "\n",
    "    display(HTML(render_table(sents_batch[i])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}