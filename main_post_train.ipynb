{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mar 6\n",
    "\n",
    "Visualize how well the class embeddings attend on words and\n",
    "sentences. The expected result would be that the “married”\n",
    "class embedding, for example, attends heavily on words and\n",
    "sentences related to marriage like “married”, “husband”, “wife”, etc.\n",
    "\n",
    "This notebook prints the results on the validation data after training\n",
    "(not during training).\n",
    "\n",
    "Also, shuffle each entity's sentences during training and weight classes\n",
    "according to class frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from random import shuffle\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.core.display import display, HTML\n",
    "from jinja2 import Template\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torch import tensor, Tensor\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from classifier import Classifier, debug\n",
    "from dao.ower.ower_dir import Sample, OwerDir\n",
    "from dao.ryn.ryn_dir import RynDir\n",
    "from util import plot_tensor\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.precision', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ower_dataset = 'ower-v3-fb-irt-3'\n",
    "ower_dir_path = Path(f'data/ower/{ower_dataset}')\n",
    "class_count = 4\n",
    "sent_count = 3\n",
    "\n",
    "ryn_dir_path = Path(f'data/ryn/irt.fb.irt.5.clean')\n",
    "\n",
    "# vectors = None\n",
    "# vectors = 'charngram.100d'\n",
    "# vectors = 'fasttext.en.300d'\n",
    "# vectors = 'fasttext.simple.300d'\n",
    "# vectors = 'glove.42B.300d'\n",
    "# vectors = 'glove.840B.300d'\n",
    "# vectors = 'glove.twitter.27B.25d'\n",
    "# vectors = 'glove.twitter.27B.50d'\n",
    "# vectors = 'glove.twitter.27B.100d'\n",
    "vectors = 'glove.twitter.27B.200d'\n",
    "# vectors = 'glove.6B.50d'\n",
    "# vectors = 'glove.6B.100d'\n",
    "# vectors = 'glove.6B.200d'\n",
    "# vectors = 'glove.6B.300d'\n",
    "\n",
    "emb_size = None\n",
    "# emb_size = 200\n",
    "\n",
    "batch_size = 1024\n",
    "sent_len = 64\n",
    "\n",
    "lr = 0.1\n",
    "epoch_count = 20\n",
    "\n",
    "log_dir = 'runs/' + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \\\n",
    "          f'_{ower_dataset}_emb-{emb_size}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Build train/valid DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ower_dir = OwerDir('OWER Dataset Directory', ower_dir_path, class_count, sent_count)\n",
    "ower_dir.check()\n",
    "\n",
    "train_set: List[Sample]\n",
    "valid_set: List[Sample]\n",
    "\n",
    "train_set, valid_set, _, vocab = ower_dir.read_datasets(vectors=vectors)\n",
    "\n",
    "#\n",
    "# Create dataloaders\n",
    "#\n",
    "\n",
    "def generate_batch(batch: List[Sample]) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "\n",
    "    ent_batch, gt_classes_batch, tok_lists_batch = zip(*batch)\n",
    "\n",
    "    cropped_tok_lists_batch = [[tok_list[:sent_len]\n",
    "                                for tok_list in tok_lists] for tok_lists in tok_lists_batch]\n",
    "\n",
    "    padded_tok_lists_batch = [[tok_list + [0] * (sent_len - len(tok_list))\n",
    "                               for tok_list in tok_lists] for tok_lists in cropped_tok_lists_batch]\n",
    "\n",
    "    for padded_tok_lists in padded_tok_lists_batch:\n",
    "        shuffle(padded_tok_lists)\n",
    "\n",
    "    return tensor(ent_batch), tensor(padded_tok_lists_batch), tensor(gt_classes_batch)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, collate_fn=generate_batch, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, collate_fn=generate_batch)\n",
    "\n",
    "#\n",
    "# Calc class freqs and set class weights\n",
    "#\n",
    "\n",
    "_, train_classes_stack, _ = zip(*train_set)\n",
    "train_classes_stack = np.array(train_classes_stack)\n",
    "train_freqs = train_classes_stack.mean(axis=0)\n",
    "\n",
    "class_weights = tensor(1 / train_freqs)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Create classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if vocab.vectors is None:\n",
    "    classifier = Classifier.from_random(len(vocab), emb_size, class_count)\n",
    "else:\n",
    "    classifier = Classifier.from_pre_trained(vocab, class_count)\n",
    "\n",
    "debug['enabled'] = True\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# criterion = MSELoss()\n",
    "criterion = BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "\n",
    "# optimizer = SGD(classifier.parameters(), lr=lr)\n",
    "optimizer = Adam(classifier.parameters(), lr=lr)\n",
    "\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "for epoch in range(epoch_count):\n",
    "\n",
    "    #\n",
    "    # Train\n",
    "    #\n",
    "\n",
    "    # Train loss across all batches\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # Valid gt/pred classes across all batches\n",
    "    train_gt_classes_stack: List[List[int]] = []\n",
    "    train_pred_classes_stack: List[List[int]] = []\n",
    "\n",
    "    for _, sents_batch, gt_classes_batch in tqdm(train_loader):\n",
    "        logits_batch = classifier(sents_batch)\n",
    "\n",
    "        loss = criterion(logits_batch, gt_classes_batch.float())\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred_classes_batch = (logits_batch > 0).int()\n",
    "\n",
    "        train_gt_classes_stack += gt_classes_batch.numpy().tolist()\n",
    "        train_pred_classes_stack += pred_classes_batch.numpy().tolist()\n",
    "\n",
    "    #\n",
    "    # Validate\n",
    "    #\n",
    "\n",
    "    # Valid loss across all batches\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    # Valid gt/pred classes across all batches\n",
    "    valid_gt_classes_stack: List[List[int]] = []\n",
    "    valid_pred_classes_stack: List[List[int]] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (ent_batch, sents_batch, gt_classes_batch) in enumerate(tqdm(valid_loader)):\n",
    "            logits_batch = classifier(sents_batch)\n",
    "\n",
    "            loss = criterion(logits_batch, gt_classes_batch.float())\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            pred_classes_batch = (logits_batch > 0).int()\n",
    "\n",
    "            valid_gt_classes_stack += gt_classes_batch.numpy().tolist()\n",
    "            valid_pred_classes_stack += pred_classes_batch.numpy().tolist()\n",
    "\n",
    "    #\n",
    "    # Log\n",
    "    #\n",
    "\n",
    "    print(f'Epoch {epoch}')\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    valid_loss /= len(valid_loader)\n",
    "\n",
    "    writer.add_scalars('loss', {'train': train_loss, 'valid': valid_loss}, epoch)\n",
    "\n",
    "    # tps = train precisions, vps = valid precisions, etc.\n",
    "    tp = precision_score(train_gt_classes_stack, train_pred_classes_stack, average=None).mean()\n",
    "    vp = precision_score(valid_gt_classes_stack, valid_pred_classes_stack, average=None).mean()\n",
    "    tr = recall_score(train_gt_classes_stack, train_pred_classes_stack, average=None).mean()\n",
    "    vr = recall_score(valid_gt_classes_stack, valid_pred_classes_stack, average=None).mean()\n",
    "    tf = f1_score(train_gt_classes_stack, train_pred_classes_stack, average=None).mean()\n",
    "    vf = f1_score(valid_gt_classes_stack, valid_pred_classes_stack, average=None).mean()\n",
    "\n",
    "    writer.add_scalars('precision', {f'train': tp, f'valid': vp}, epoch)\n",
    "    writer.add_scalars('recall', {f'train': tr, f'valid': vr}, epoch)\n",
    "    writer.add_scalars('f1', {f'train': tf, f'valid': vf}, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4 Calc top class-word attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class_embs = classifier.class_embs\n",
    "tok_embs = classifier.embedding_bag.weight\n",
    "\n",
    "tok_atts = torch.einsum('ce, ve -> cv', class_embs, tok_embs)\n",
    "result = tok_atts.sort(descending=True)\n",
    "indices = result.indices.numpy()\n",
    "values = result.values.detach().numpy()\n",
    "\n",
    "class_labels = ['male', 'married', 'American', 'actor']\n",
    "\n",
    "for c, c_lbl in zip(range(class_count), class_labels):\n",
    "    print(c_lbl)\n",
    "    for tok, val in zip(indices[c][:10], values[c][:10]):\n",
    "        print('\\t{} ({:.2f})'.format(vocab.itos[tok], val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Visualize class-word attentions in sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tok_atts /= max(-tok_atts.min(), tok_atts.max())\n",
    "tok_atts *= 512\n",
    "tok_atts += 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_color(att: float) -> str:\n",
    "    att = max(min(att, 255), 0)\n",
    "    r, g, b = pyplot.get_cmap('viridis').colors[int(att)]\n",
    "\n",
    "    return f'rgba({int(r * 256)}, {int(g * 256)}, {int(b * 256)}, 0.5)'\n",
    "\n",
    "\n",
    "def render_sent(class_: int, sent: List[int]) -> str:\n",
    "\n",
    "    words = ['<span style=\"background-color: {}\">{}</span>'.format(\n",
    "                get_color(tok_atts[class_][tok]),\n",
    "                vocab.itos[tok] if tok != 0 else '_'\n",
    "            ) for tok in sent]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def render_table(sents: List[List[int]]) -> None:\n",
    "    display(HTML(Template('''\n",
    "        <style>\n",
    "            table.atts td { text-align: left }\n",
    "        </style>\n",
    "\n",
    "        <table class='atts'>\n",
    "            <tr>\n",
    "                <th></th>\n",
    "\n",
    "                {% for i in range(len(sents)) %}\n",
    "                <th> Sent {{ i }} </th>\n",
    "                {% endfor %}\n",
    "            </tr>\n",
    "\n",
    "            {% for c in range(len(class_labels)) %}\n",
    "            <tr>\n",
    "                <th>{{ class_labels[c] }}</th>\n",
    "\n",
    "                {% for sent in sents %}\n",
    "                <td>{{ render_sent(c, sent) }}</td>\n",
    "                {% endfor %}\n",
    "            </tr>\n",
    "            {% endfor %}\n",
    "        </table>\n",
    "    ''').render(\n",
    "        sents=sents,\n",
    "        class_labels=class_labels,\n",
    "        render_sent=render_sent,\n",
    "        len=len\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ryn_dir = RynDir('Ryn Directory', ryn_dir_path)\n",
    "ryn_dir.check()\n",
    "rid_to_label = ryn_dir.split_dir.entity_labels_txt.load_rid_to_label()\n",
    "\n",
    "for i in range(20):\n",
    "\n",
    "    ent = ent_batch[i].item()\n",
    "\n",
    "    display(HTML('<h1>{} ({})</h1>'.format(rid_to_label[ent], ent)))\n",
    "\n",
    "    texts = [' '.join([vocab.itos[tok] if tok != 0 else '_' for tok in tok_list])\n",
    "             for tok_list in sents_batch[i]]\n",
    "\n",
    "    display(HTML(Template('''\n",
    "        <ul>\n",
    "            {% for text in texts %}\n",
    "                <li> {{ text }} </li>\n",
    "            {% endfor %}\n",
    "        </ul>\n",
    "    ''').render(texts=texts)))\n",
    "\n",
    "    print('male', 'married', 'American', 'actor')\n",
    "    print('logits =', logits_batch[i].detach().numpy())\n",
    "    print('pred =', pred_classes_batch[i].detach().numpy())\n",
    "    print('gt =', gt_classes_batch[i].detach().numpy())\n",
    "\n",
    "    class_labels = ['male', 'married', 'American', 'actor']\n",
    "    sent_labels = [f'sent {i}' for i in range(sent_count)]\n",
    "    plot_tensor(debug['atts_batch'][i], 'atts', [class_labels, sent_labels])\n",
    "\n",
    "    display(HTML(render_table(sents_batch[i])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}