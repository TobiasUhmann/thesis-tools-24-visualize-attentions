{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Visualize how well the class embeddings attend on words and\n",
    "sentences. The expected result would be that the “married”\n",
    "class embedding, for example, attends heavily on words and\n",
    "sentences related to marriage like “married”, “husband”, “wife”, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from data.power.samples.samples_dir import SamplesDir\n",
    "from data.power.texter_pkl import TexterPkl\n",
    "\n",
    "from pathlib import Path\n",
    "from random import shuffle\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.core.display import display, HTML\n",
    "from jinja2 import Template\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torch import tensor, Tensor\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dao.ower.ower_dir import Sample, OwerDir\n",
    "from util import plot_tensor\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.precision', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "texter_pkl_path = 'data/power/texter/context-power-v2/cde-irt-5-marked.pkl'\n",
    "\n",
    "# Input data\n",
    "samples_dir_path = 'data/power/samples-v5/cde-irt-5-clean/'\n",
    "class_count = 8\n",
    "sent_count = 5\n",
    "\n",
    "# Pre-processing\n",
    "sent_len = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tobias\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchtext\\data\\field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "C:\\Users\\Tobias\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchtext\\data\\example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "C:\\Users\\Tobias\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchtext\\data\\example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      ".vector_cache\\glove.6B.zip:  10%|▉         | 83.9M/862M [07:49<1:12:40, 178kB/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-5-60cbbfe793bb>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     12\u001B[0m     \u001B[0mtrain_set\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalid_set\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvocab\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mower_dir\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_datasets\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mclass_count\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msent_count\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 14\u001B[1;33m     \u001B[0mtrain_set\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalid_set\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvocab\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mower_dir\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_datasets\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mclass_count\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msent_count\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvectors\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     15\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[1;31m## Create dataloaders\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\HSRM\\Master\\Thesis\\dev\\24-visualize-attentions\\src\\dao\\ower\\ower_dir.py\u001B[0m in \u001B[0;36mread_datasets\u001B[1;34m(self, class_count, sent_count, vectors)\u001B[0m\n\u001B[0;32m    122\u001B[0m         \u001B[1;31m#\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    123\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 124\u001B[1;33m         \u001B[0msent_field\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbuild_vocab\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_tab_set\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvectors\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mvectors\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    125\u001B[0m         \u001B[0mvocab\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msent_field\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvocab\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    126\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchtext\\data\\field.py\u001B[0m in \u001B[0;36mbuild_vocab\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    304\u001B[0m                             self.eos_token] + kwargs.pop('specials', [])\n\u001B[0;32m    305\u001B[0m             if tok is not None))\n\u001B[1;32m--> 306\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvocab\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvocab_cls\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcounter\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mspecials\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mspecials\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    307\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    308\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mnumericalize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchtext\\vocab.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, counter, max_size, min_freq, specials, vectors, unk_init, vectors_cache, specials_first)\u001B[0m\n\u001B[0;32m     97\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvectors\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     98\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mvectors\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 99\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload_vectors\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvectors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0munk_init\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0munk_init\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcache\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mvectors_cache\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    100\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    101\u001B[0m             \u001B[1;32massert\u001B[0m \u001B[0munk_init\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mvectors_cache\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchtext\\vocab.py\u001B[0m in \u001B[0;36mload_vectors\u001B[1;34m(self, vectors, **kwargs)\u001B[0m\n\u001B[0;32m    182\u001B[0m                         \"vectors are {}\".format(\n\u001B[0;32m    183\u001B[0m                             vector, list(pretrained_aliases.keys())))\n\u001B[1;32m--> 184\u001B[1;33m                 \u001B[0mvectors\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpretrained_aliases\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mvector\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    185\u001B[0m             \u001B[1;32melif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvector\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mVectors\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    186\u001B[0m                 raise ValueError(\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchtext\\vocab.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, name, dim, **kwargs)\u001B[0m\n\u001B[0;32m    485\u001B[0m         \u001B[0murl\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0murl\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    486\u001B[0m         \u001B[0mname\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'glove.{}.{}d.txt'\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdim\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 487\u001B[1;33m         \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mGloVe\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0murl\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    488\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    489\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchtext\\vocab.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, name, cache, url, unk_init, max_vectors)\u001B[0m\n\u001B[0;32m    324\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdim\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    325\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munk_init\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0munk_init\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0munk_init\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 326\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcache\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcache\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0murl\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax_vectors\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmax_vectors\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    327\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    328\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__getitem__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtoken\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchtext\\vocab.py\u001B[0m in \u001B[0;36mcache\u001B[1;34m(self, name, cache, url, max_vectors)\u001B[0m\n\u001B[0;32m    362\u001B[0m                         \u001B[1;32mexcept\u001B[0m \u001B[0mKeyboardInterrupt\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# remove the partial zip file\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    363\u001B[0m                             \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mremove\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdest\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 364\u001B[1;33m                             \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    365\u001B[0m                 \u001B[0mlogger\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'Extracting vectors into {}'\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcache\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    366\u001B[0m                 \u001B[0mext\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplitext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdest\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchtext\\vocab.py\u001B[0m in \u001B[0;36mcache\u001B[1;34m(self, name, cache, url, max_vectors)\u001B[0m\n\u001B[0;32m    359\u001B[0m                     \u001B[1;32mwith\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0munit\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'B'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0munit_scale\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mminiters\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdesc\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdest\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    360\u001B[0m                         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 361\u001B[1;33m                             \u001B[0murlretrieve\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdest\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreporthook\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mreporthook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mt\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    362\u001B[0m                         \u001B[1;32mexcept\u001B[0m \u001B[0mKeyboardInterrupt\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# remove the partial zip file\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    363\u001B[0m                             \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mremove\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdest\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\tobias\\appdata\\local\\programs\\python\\python39\\lib\\urllib\\request.py\u001B[0m in \u001B[0;36murlretrieve\u001B[1;34m(url, filename, reporthook, data)\u001B[0m\n\u001B[0;32m    266\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    267\u001B[0m             \u001B[1;32mwhile\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 268\u001B[1;33m                 \u001B[0mblock\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    269\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mblock\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    270\u001B[0m                     \u001B[1;32mbreak\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\tobias\\appdata\\local\\programs\\python\\python39\\lib\\http\\client.py\u001B[0m in \u001B[0;36mread\u001B[1;34m(self, amt)\u001B[0m\n\u001B[0;32m    456\u001B[0m             \u001B[1;31m# Amount is given, implement using readinto\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    457\u001B[0m             \u001B[0mb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbytearray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mamt\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 458\u001B[1;33m             \u001B[0mn\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreadinto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mb\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    459\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mmemoryview\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mb\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0mn\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtobytes\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    460\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\tobias\\appdata\\local\\programs\\python\\python39\\lib\\http\\client.py\u001B[0m in \u001B[0;36mreadinto\u001B[1;34m(self, b)\u001B[0m\n\u001B[0;32m    500\u001B[0m         \u001B[1;31m# connection, and the user is reading more bytes than will be provided\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    501\u001B[0m         \u001B[1;31m# (for example, reading in 1k chunks)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 502\u001B[1;33m         \u001B[0mn\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreadinto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mb\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    503\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mn\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mb\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    504\u001B[0m             \u001B[1;31m# Ideally, we would raise IncompleteRead if the content-length\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\tobias\\appdata\\local\\programs\\python\\python39\\lib\\socket.py\u001B[0m in \u001B[0;36mreadinto\u001B[1;34m(self, b)\u001B[0m\n\u001B[0;32m    702\u001B[0m         \u001B[1;32mwhile\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    703\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 704\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_sock\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrecv_into\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mb\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    705\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    706\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_timeout_occurred\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#\n",
    "# Check that (input) POWER Texter PKL exists\n",
    "#\n",
    "\n",
    "texter_pkl = TexterPkl(Path(texter_pkl_path))\n",
    "texter_pkl.check()\n",
    "\n",
    "#\n",
    "# Check that (input) POWER Samples Directory exists\n",
    "#\n",
    "\n",
    "samples_dir = SamplesDir(Path(samples_dir_path))\n",
    "samples_dir.check()\n",
    "\n",
    "## Load datasets\n",
    "\n",
    "train_set: List[Sample]\n",
    "valid_set: List[Sample]\n",
    "\n",
    "if emb_size is not None:\n",
    "    train_set, valid_set, _, vocab = ower_dir.read_datasets(class_count, sent_count)\n",
    "else:\n",
    "    train_set, valid_set, _, vocab = ower_dir.read_datasets(class_count, sent_count, vectors)\n",
    "\n",
    "## Create dataloaders\n",
    "\n",
    "def generate_batch(batch: List[Sample]) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "\n",
    "    ent_batch, gt_classes_batch, tok_lists_batch = zip(*batch)\n",
    "\n",
    "    cropped_tok_lists_batch = [[tok_list[:sent_len]\n",
    "                                for tok_list in tok_lists] for tok_lists in tok_lists_batch]\n",
    "\n",
    "    padded_tok_lists_batch = [[tok_list + [0] * (sent_len - len(tok_list))\n",
    "                               for tok_list in tok_lists] for tok_lists in cropped_tok_lists_batch]\n",
    "\n",
    "    for padded_tok_lists in padded_tok_lists_batch:\n",
    "        shuffle(padded_tok_lists)\n",
    "\n",
    "    return tensor(ent_batch), tensor(padded_tok_lists_batch), tensor(gt_classes_batch)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, collate_fn=generate_batch, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Build classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Calc class weights\n",
    "\n",
    "_, train_classes_stack, _ = zip(*train_set)\n",
    "train_classes_stack = numpy.array(train_classes_stack)\n",
    "train_freqs = train_classes_stack.mean(axis=0)\n",
    "\n",
    "class_weights = tensor(1 / train_freqs).to(device)\n",
    "\n",
    "## Create classifier\n",
    "\n",
    "if emb_size is not None:\n",
    "    classifier = Classifier.from_random(len(vocab), emb_size, class_count).to(device)\n",
    "else:\n",
    "    classifier = Classifier.from_pre_trained(vocab, class_count).to(device)\n",
    "\n",
    "debug['enabled'] = True\n",
    "\n",
    "optimizer = Adam(classifier.parameters(), lr=lr)\n",
    "criterion = BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "\n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "log_first_batch = False\n",
    "\n",
    "for epoch in range(epoch_count):\n",
    "\n",
    "    ## Train\n",
    "\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # Valid gt/pred classes across all batches\n",
    "    train_gt_classes_stack: List[List[int]] = []\n",
    "    train_pred_classes_stack: List[List[int]] = []\n",
    "\n",
    "    for batch_idx, (_, sents_batch, gt_classes_batch) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch}')):\n",
    "        sents_batch = sents_batch.to(device)\n",
    "        gt_classes_batch = gt_classes_batch.to(device)\n",
    "\n",
    "        logits_batch = classifier(sents_batch)\n",
    "\n",
    "        loss = criterion(logits_batch, gt_classes_batch.float())\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred_classes_batch = (logits_batch > 0).int()\n",
    "\n",
    "        train_gt_classes_stack += gt_classes_batch.cpu().numpy().tolist()\n",
    "        train_pred_classes_stack += pred_classes_batch.cpu().numpy().tolist()\n",
    "\n",
    "        #\n",
    "        # Log first batch\n",
    "        #\n",
    "\n",
    "        if log_first_batch and batch_idx == 0:\n",
    "\n",
    "            dlb = logits_batch.cpu().detach().numpy()  # logits batch\n",
    "            dpb = pred_classes_batch.cpu().detach().numpy()  # predicted classes batch\n",
    "            dgb = gt_classes_batch.cpu().detach().numpy()  # ground truth classes batch\n",
    "            dsb = sents_batch.cpu().detach().numpy()  # sentences batch\n",
    "\n",
    "            df_cols = ['entity', 'logits', 'p', 'gt', 'sents']\n",
    "            df_data = [('foo', logits, pred_classes, classes, [[vocab.itos[tok] for tok in sent] for sent in sents])\n",
    "                       for logits, pred_classes, classes, sents in zip(dlb, dpb, dgb, dsb)]\n",
    "\n",
    "            df = pd.DataFrame(df_data[:8], columns=df_cols)\n",
    "            display(df)\n",
    "\n",
    "            display_atts = debug['atts_batch'][:8].cpu()\n",
    "            ent_labels = [f'ent {i}' for i in range(batch_size)]\n",
    "            class_labels = [f'clss {i}' for i in range(class_count)]\n",
    "            sent_labels = [f'sent {i}' for i in range(sent_count)]\n",
    "            plot_tensor(display_atts, 'atts_batch', [ent_labels, class_labels, sent_labels])\n",
    "\n",
    "    ## Validate\n",
    "\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    # Valid gt/pred classes across all batches\n",
    "    valid_gt_classes_stack: List[List[int]] = []\n",
    "    valid_pred_classes_stack: List[List[int]] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (ent_batch, sents_batch, gt_classes_batch) in enumerate(tqdm(valid_loader, desc=f'Epoch {epoch}')):\n",
    "            sents_batch = sents_batch.to(device)\n",
    "            gt_classes_batch = gt_classes_batch.to(device)\n",
    "\n",
    "            logits_batch = classifier(sents_batch)\n",
    "\n",
    "            loss = criterion(logits_batch, gt_classes_batch.float())\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            pred_classes_batch = (logits_batch > 0).int()\n",
    "\n",
    "            valid_gt_classes_stack += gt_classes_batch.cpu().numpy().tolist()\n",
    "            valid_pred_classes_stack += pred_classes_batch.cpu().numpy().tolist()\n",
    "\n",
    "            #\n",
    "            # Print first batch\n",
    "            #\n",
    "\n",
    "            if log_first_batch and batch_idx == 0:\n",
    "\n",
    "                dlb = logits_batch.cpu().detach().numpy()  # logits batch\n",
    "                dpb = pred_classes_batch.cpu().detach().numpy()  # predicted classes batch\n",
    "                dgb = gt_classes_batch.cpu().detach().numpy()  # ground truth classes batch\n",
    "                dsb = sents_batch.cpu().detach().numpy()  # sentences batch\n",
    "\n",
    "                df_cols = ['entity', 'logits', 'p', 'gt', 'sents']\n",
    "                df_data = [('foo', logits, pred_classes, classes, [[vocab.itos[tok] for tok in sent] for sent in sents])\n",
    "                           for logits, pred_classes, classes, sents in zip(dlb, dpb, dgb, dsb)]\n",
    "\n",
    "                df = pd.DataFrame(df_data[:8], columns=df_cols)\n",
    "                display(df)\n",
    "\n",
    "                display_atts = debug['atts_batch'][:8].cpu()\n",
    "                ent_labels = [f'ent {i}' for i in range(batch_size)]\n",
    "                class_labels = [f'clss {i}' for i in range(class_count)]\n",
    "                sent_labels = [f'sent {i}' for i in range(sent_count)]\n",
    "                plot_tensor(display_atts, 'atts_batch', [ent_labels, class_labels, sent_labels])\n",
    "\n",
    "\n",
    "    ## Log loss\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    valid_loss /= len(valid_loader)\n",
    "\n",
    "    writer.add_scalars('loss', {'train': train_loss, 'valid': valid_loss}, epoch)\n",
    "\n",
    "    ## Log metrics for most/least common classes\n",
    "\n",
    "    # tps = train precisions, vps = valid precisions, etc.\n",
    "    tps = precision_score(train_gt_classes_stack, train_pred_classes_stack, average=None)\n",
    "    vps = precision_score(valid_gt_classes_stack, valid_pred_classes_stack, average=None)\n",
    "    trs = recall_score(train_gt_classes_stack, train_pred_classes_stack, average=None)\n",
    "    vrs = recall_score(valid_gt_classes_stack, valid_pred_classes_stack, average=None)\n",
    "    tfs = f1_score(train_gt_classes_stack, train_pred_classes_stack, average=None)\n",
    "    vfs = f1_score(valid_gt_classes_stack, valid_pred_classes_stack, average=None)\n",
    "\n",
    "    # Log metrics for each class c\n",
    "    for c, (tp, vp, tr, vr, tf, vf), in enumerate(zip(tps, vps, trs, vrs, tfs, vfs)):\n",
    "\n",
    "        # many classes -> log only first and last ones\n",
    "        if (class_count > 2 * 3) and (3 <= c <= len(tps) - 3 - 1):\n",
    "            continue\n",
    "\n",
    "        writer.add_scalars('precision', {f'train_{c}': tp}, epoch)\n",
    "        writer.add_scalars('precision', {f'valid_{c}': vp}, epoch)\n",
    "        writer.add_scalars('recall', {f'train_{c}': tr}, epoch)\n",
    "        writer.add_scalars('recall', {f'valid_{c}': vr}, epoch)\n",
    "        writer.add_scalars('f1', {f'train_{c}': tf}, epoch)\n",
    "        writer.add_scalars('f1', {f'valid_{c}': vf}, epoch)\n",
    "\n",
    "    ## Log macro metrics over all classes\n",
    "\n",
    "    # mtp = mean train precision, mvp = mean valid precision, etc.\n",
    "    mtp = tps.mean()\n",
    "    mvp = vps.mean()\n",
    "    mtr = trs.mean()\n",
    "    mvr = vrs.mean()\n",
    "    mtf = tfs.mean()\n",
    "    mvf = vfs.mean()\n",
    "\n",
    "    writer.add_scalars('precision', {'train': mtp}, epoch)\n",
    "    writer.add_scalars('precision', {'valid': mvp}, epoch)\n",
    "    writer.add_scalars('recall', {'train': mtr}, epoch)\n",
    "    writer.add_scalars('recall', {'valid': mvr}, epoch)\n",
    "    writer.add_scalars('f1', {'train': mtf}, epoch)\n",
    "    writer.add_scalars('f1', {'valid': mvf}, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calc top class-word attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class_embs = classifier.class_embs\n",
    "tok_embs = classifier.embedding_bag.weight\n",
    "\n",
    "tok_atts = torch.einsum('ce, ve -> cv', class_embs, tok_embs)\n",
    "result = tok_atts.sort(descending=True)\n",
    "indices = result.indices.cpu().numpy()\n",
    "values = result.values.cpu().detach().numpy()\n",
    "\n",
    "rel_tail_freq_lbl_tuples = ower_dir.classes_tsv.load()\n",
    "_, _, _, class_labels = zip(*rel_tail_freq_lbl_tuples)\n",
    "\n",
    "for c, c_lbl in zip(range(class_count), class_labels):\n",
    "    print('\\n', c_lbl)\n",
    "    for tok, val in zip(indices[c][:10], values[c][:10]):\n",
    "        print('\\t{} ({:.2f})'.format(vocab.itos[tok], val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize class-word attentions in sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tok_atts /= max(-tok_atts.min(), tok_atts.max())\n",
    "tok_atts *= 512\n",
    "tok_atts += 128\n",
    "\n",
    "def get_color(att: float) -> str:\n",
    "    att = max(min(att, 255), 0)\n",
    "    r, g, b = pyplot.get_cmap('viridis').colors[int(att)]\n",
    "\n",
    "    return f'rgba({int(r * 256)}, {int(g * 256)}, {int(b * 256)}, 0.5)'\n",
    "\n",
    "\n",
    "def render_sent(class_: int, sent: List[int]) -> str:\n",
    "\n",
    "    words = ['<span style=\"background-color: {}\">{}</span>'.format(\n",
    "                get_color(tok_atts[class_][tok]),\n",
    "                vocab.itos[tok] if tok != 0 else '_'\n",
    "            ) for tok in sent]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def render_table(sents: List[List[int]]) -> None:\n",
    "    short_class_labels = [class_label[-20:] for class_label in class_labels]\n",
    "\n",
    "    display(HTML(Template('''\n",
    "        <style>\n",
    "            table.atts td { text-align: left }\n",
    "        </style>\n",
    "\n",
    "        <table class='atts'>\n",
    "            <tr>\n",
    "                <th></th>\n",
    "\n",
    "                {% for i in range(len(sents)) %}\n",
    "                <th> Sent {{ i }} </th>\n",
    "                {% endfor %}\n",
    "            </tr>\n",
    "\n",
    "            {% for c in range(len(class_labels)) %}\n",
    "            <tr>\n",
    "                <th>{{ class_labels[c] }}</th>\n",
    "\n",
    "                {% for sent in sents %}\n",
    "                <td>{{ render_sent(c, sent) }}</td>\n",
    "                {% endfor %}\n",
    "            </tr>\n",
    "            {% endfor %}\n",
    "        </table>\n",
    "    ''').render(\n",
    "        sents=sents,\n",
    "        class_labels=short_class_labels,\n",
    "        render_sent=render_sent,\n",
    "        len=len\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ent_to_lbl = ower_dir.ent_labels_txt.load()\n",
    "\n",
    "logits_batch = logits_batch.cpu()\n",
    "pred_classes_batch = pred_classes_batch.cpu()\n",
    "gt_classes_batch = gt_classes_batch.cpu()\n",
    "atts_batch = debug['atts_batch'].cpu()\n",
    "\n",
    "for i in range(20):\n",
    "\n",
    "    ent = ent_batch[i].item()\n",
    "\n",
    "    display(HTML('<h1>{} ({})</h1>'.format(ent_to_lbl[ent], ent)))\n",
    "\n",
    "    texts = [' '.join([vocab.itos[tok] if tok != 0 else '_' for tok in tok_list])\n",
    "             for tok_list in sents_batch[i]]\n",
    "\n",
    "    display(HTML(Template('''\n",
    "        <ul>\n",
    "            {% for text in texts %}\n",
    "                <li> {{ text }} </li>\n",
    "            {% endfor %}\n",
    "        </ul>\n",
    "    ''').render(texts=texts)))\n",
    "\n",
    "    print(class_labels)\n",
    "    print('logits =', logits_batch[i])\n",
    "    print('pred =', pred_classes_batch[i])\n",
    "    print('gt =', gt_classes_batch[i])\n",
    "\n",
    "    sent_labels = [f'sent {i}' for i in range(sent_count)]\n",
    "    plot_tensor(atts_batch[i], 'atts', [class_labels, sent_labels])\n",
    "\n",
    "    display(HTML(render_table(sents_batch[i])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}